{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxV6hkWBnhXagk6WLkRygK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zanthegreat18/DataCrawling/blob/main/CrawlingDataKompas_com.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSspacVQWPVB",
        "outputId": "a9ccca4d-09a5-4746-8f68-f65e5b01334d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.11/dist-packages (4.12.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n",
            "[✓] Berhasil simpan: Pesawat Jatuh ke Ladang di Copake New York, 6 Penumpang Tewas\n",
            "[✓] Berhasil simpan: Israel Serang Rumah Sakit Terakhir yang Berfungsi di Gaza hingga Porak-poranda\n",
            "[✓] Berhasil simpan: Nissan GT-R Nismo Disita Kejagung, Simak Detailnya!\n",
            "✅ Semua berita berhasil disimpan ke MongoDB.\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas\n",
        "!pip install pymongo\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pymongo\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from pymongo import MongoClient\n",
        "import time\n",
        "\n",
        "#1. Koneksi ke mongoDB\n",
        "\n",
        "client = MongoClient(\"mongodb+srv://Ojan123:Ojan123@ojanmenolaklupa.lgx84xs.mongodb.net/\")\n",
        "db = client['Crawling_news']\n",
        "collection = db['kompas']\n",
        "\n",
        "# --------------------------\n",
        "# 2. Header & URL\n",
        "# --------------------------\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
        "}\n",
        "url = 'https://www.kompas.com/'\n",
        "\n",
        "# --------------------------\n",
        "# 3. Ambil Halaman Utama\n",
        "# --------------------------\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "# Periksa apakah halaman dapat diakses\n",
        "if response.status_code != 200:\n",
        "    print(f\"Error: Tidak dapat mengakses halaman utama (Status code: {response.status_code})\")\n",
        "else:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    berita_list = soup.find_all('div', class_='article__list__title')\n",
        "\n",
        "    # --------------------------\n",
        "    # 4. Loop dan Simpan ke MongoDB\n",
        "    # --------------------------\n",
        "    for berita in berita_list:\n",
        "        link_tag = berita.find('a')\n",
        "        if not link_tag:\n",
        "            continue\n",
        "\n",
        "        title = link_tag.get_text(strip=True)\n",
        "        url_berita = link_tag['href']\n",
        "\n",
        "        # Ambil halaman isi berita\n",
        "        berita_response = requests.get(url_berita, headers=headers)\n",
        "\n",
        "        # Periksa status code untuk artikel\n",
        "        if berita_response.status_code != 200:\n",
        "            print(f\"Error: Tidak dapat mengakses artikel {url_berita} (Status code: {berita_response.status_code})\")\n",
        "            continue\n",
        "\n",
        "        berita_soup = BeautifulSoup(berita_response.text, 'html.parser')\n",
        "\n",
        "        # Ambil tanggal\n",
        "        tanggal_meta = berita_soup.find('meta', attrs={'property': 'article:published_time'})\n",
        "        tanggal = tanggal_meta['content'] if tanggal_meta else \"Tidak ditemukan\"\n",
        "\n",
        "        # Ambil isi berita\n",
        "        content_div = berita_soup.find('div', class_='read__content')  # kadang juga 'article__body'\n",
        "        content = content_div.get_text(separator=' ', strip=True) if content_div else \"Tidak ditemukan\"\n",
        "\n",
        "        # Buat dokumen MongoDB\n",
        "        doc = {\n",
        "            'url': url_berita,\n",
        "            'title': title,\n",
        "            'content': content,\n",
        "            'date': tanggal,\n",
        "            'crawled_at': datetime.utcnow().isoformat(),\n",
        "            'source': 'kompas.com',\n",
        "            'author': None,\n",
        "            'editor': None,\n",
        "            'category': None,\n",
        "            'tags': []\n",
        "        }\n",
        "\n",
        "        # Simpan (hindari duplikat dengan 'url' sebagai key)\n",
        "        collection.update_one({'url': url_berita}, {'$set': doc}, upsert=True)\n",
        "        print(f\"[✓] Berhasil simpan: {title}\")\n",
        "\n",
        "        time.sleep(1)  # jeda supaya tidak dianggap spam\n",
        "\n",
        "    print(\"✅ Semua berita berhasil disimpan ke MongoDB.\")"
      ]
    }
  ]
}